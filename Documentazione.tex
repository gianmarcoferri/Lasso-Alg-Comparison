\documentclass[12pt,a4paper]{article}
\usepackage[italian]{babel}
%\usepackage[T1]{fontenc} % Riga da commentare se si compila con PDFLaTeX
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{lipsum} % genera testo fittizio
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{titlesec}
\usepackage{cochineal}
\usepackage[cochineal]{newtxmath}
\usepackage[simplified]{pgf-umlcd}
\usepackage{float}
\usepackage{wrapfig,lipsum}
\usepackage{fancyhdr, etoolbox}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{xparse}
\usepackage{url}
\usepackage{setspace}
\usepackage{longtable}
\usepackage{etaremune}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage[edges]{forest}
\usepackage{spverbatim}
\usepackage{etoolbox}
\usepackage{comment}
\usepackage{subcaption}

\makeatletter
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
\makeatother

\definecolor{folderbg}{RGB}{124,166,198}
\definecolor{folderborder}{RGB}{110,144,169}
\newlength\Size
\setlength\Size{4pt}

%\titleformat{\chapter}[display]{\Huge\bfseries}{}{0pt}{\thechapter.\ }
\titleformat{\chapter}{\bfseries\Large}{\arabic{chapter}.~}{0pt}{}


\graphicspath{{figures/}}
%
%\addtolength{\topmargin}{-.875in} % reduce the default top margin
%\addtolength{\topmargin}{-2cm} % reduce the default top margin
%

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000


\def\code#1{\texttt{#1}}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                %
%     Begin Docuemnt [start]     %
%                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\newgeometry{margin=1in}
\begin{titlepage}
	\centering
	\includegraphics[width=0.5\textwidth]{logounipg}\par\vspace{1cm}
	\large{Progetto di}\par
	\large{\textbf{Signal Processing and Optimization for Big Data}}\par
	\small{Corso di Laurea in Ingegneria Informatica e Robotica} \par
    \small{Curriculum Data Science}\par
	\textsc{\small{Dipartimento di Ingegneria}}\par

	%\vfill
	\vspace{0.5cm}
	docente\par
	Prof.~Paolo \textsc{Banelli}

	\vspace{1cm}
	\vspace{3cm}
	\textbf{\huge{Studio Comparativo degli Algoritmi di Regressione Lasso in MATLAB}}\par
	\vspace{0.2cm}
    %Sottotitolo 
    %\par
    %\textbf{Esplorazione interattiva delle relazioni di Game of Thrones}
    %\par
	\vspace{8cm}

	%\large{studenti}\par
	\vspace{0.2cm}
	\begin{tabular}{ l l l l }
	\large{363433} & \large{\textbf{Gian Marco}} & \large{\textbf{Ferri}} & \large{gianmarco.ferri@studenti.unipg.it}\\
	\end{tabular}
    \vspace{0.5cm}
    \newline
    %\textbf{Link GitHub: \url{https://github.com/Gian99Marco/House-Price-Prediction}}\par

	\vfill
	% Bottom of the page
	%{\large \today\par}
	%\raggedright
	%\small{Data ultimo aggiornamento: \today}
\end{titlepage}
% Ends the declared geometry for the titlepage
\restoregeometry
\normalfont
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Title Page [end]     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage %\thispagestyle{empty} \ \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Indice [start]     %
%%%%%%%%%%%%%%%%%%%%%%%%%%

%\onehalfspacing
\tableofcontents

\newpage










%%%%%%%%%%%%%%%%%%%%%%%%
%     Indice [end]     %
%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduzione}
Questo progetto affronta la risoluzione del problema della regressione con regolarizzazione Lasso tramite l’implementazione e il confronto di diversi algoritmi. In particolare, sono stati sviluppati in Matlab tre approcci distinti: ISTA, ADMM e una versione simulata di ADMM distribuito su più agenti. \\

Nella parte finale della documentazione vengono presentati dei confronti tra i tre algoritmi in termini di tempi di calcolo, numero di iterazioni e grafici che illustrano l’andamento della convergenza durante le iterazioni.

\subsection{Contesto Generale}
La regressione LASSO (Least Absolute Shrinkage and Selection Operator) è una tecnica fondamentale nel campo della modellistica statistica e del machine learning. \par
Questa metodologia permette di affrontare problemi come l’overfitting e la selezione delle variabili, offrendo una strategia efficace per sviluppare modelli affidabili e generalizzabili.\\

Con la crescente complessità degli algoritmi, il rischio di overfitting, ossia la tendenza di un modello a modellare troppo fedelmente i dati di addestramento includendo anche il rumore, è diventato sempre più rilevante. Il metodo di regolarizzazione LASSO affronta tale problema introducendo una penalizzazione sulla somma dei valori assoluti dei coefficienti delle variabili nel modello. L'intensità di questa penalità è regolata da un iperparametro.\par
La caratteristica distintiva della regressione LASSO è proprio l’utilizzo della regolarizzazione di tipo $L1$, che incentiva la sparsità nei coefficienti stimati. In pratica, ciò comporta che LASSO tende ad azzerare molti coefficienti, selezionando automaticamente solo le variabili più significative per la previsione.\par
Questa sua proprietà la differenzia da altre tecniche come la regressione Ridge, che usa una penalità $L2$, e la rende particolarmente utile quando si ha a che fare con dataset che hanno un numero elevato di feature.



\newpage

\section{Analisi Teorica}
In questo capitolo si propone un’analisi dettagliata del problema dal punto di vista sia teorico che matematico, con l’obiettivo di approfondire la comprensione degli algoritmi che verranno implementati in seguito.\par
Viene presentata una descrizione esaustiva della regressione LASSO, introducendo il concetto della regolarizzazione L1 e discutendo le soluzioni teoriche relative al problema di ottimizzazione collegato.\par
Si prendono inoltre in considerazione le principali varianti dell’algoritmo, ovvero ISTA, ADMM e ADMM distribuito, illustrandone le basi teoriche necessarie per comprendere appieno le applicazioni pratiche che saranno affrontate nei capitoli successivi.


\subsection{LASSO}
Il LASSO (Least Absolute Shrinkage and Selection Operator) è un metodo di regressione che applica una penalizzazione di tipo $L1$ alla funzione di ottimizzazione. Tale penalità, proporzionale alla somma dei valori assoluti dei coefficienti, induce sparsità nel vettore dei parametri, portando molti di essi ad annullarsi. Questo meccanismo consente non solo di selezionare automaticamente le variabili più rilevanti, ma anche di ottenere un modello più semplice, riducendo efficacemente il fenomeno dell'overfitting. \\

Tale risultato è ottenuto risolvendo un problema di ottimizzazione che coinvolge la minimizzazione di una funzione di costo, la quale comprende sia il termine di regressione che la penalità $L1$.

\[
\min_{\mathbf{x}} \frac{1}{2} \|\mathbf{y} - A\mathbf{x}\|_2^2 + \alpha \|\mathbf{x}\|_1
\]

L’espressione riportata descrive il problema di ottimizzazione relativo alla regressione LASSO.
Nell’equazione, $\mathbf{x}$ indica il vettore dei coefficienti del modello, $\mathbf{y}$ rappresenta il vettore delle variabili dipendenti, $A$ corrisponde alla matrice delle variabili indipendenti (features) e $\alpha$ è l'iperparametro che controlla l’intensità della regolarizzazione.

\subsection{Soft-Thresholding}
Nell’algoritmo ISTA (Iterative Shrinkage Thresholding Algorithm), la regressione LASSO viene risolta utilizzando un approccio iterativo basato sul Gradient Descent. \par
In questo contesto, il Gradient Descent viene impiegato per affrontare il problema di ottimizzazione precedentemente illustrato, aggiornando progressivamente i pesi del modello nella direzione opposta al gradiente della funzione costo, fino al raggiungimento della convergenza.


\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \gamma \nabla \left[ \frac{1}{2} \|\mathbf{y} - A\mathbf{x}^{(k)}\|_2^2 + \alpha \|\mathbf{x}^{(k)}\|_1 \right]
\]

Poiché la norma $L_1$ non è differenziabile, si ricorre al concetto di sub-gradiente attraverso l’impiego dell’operatore di Soft-Thresholding.

\[
[\text{sub} \nabla f(x_0)]_i = 
\begin{cases} 
\frac{\partial f(x_0)}{\partial x_i}, & \text{se differenziabile in } x_0 \\
\left[\frac{\partial f(x_0^-)}{\partial x_i}, \frac{\partial f(x_0^+)}{\partial x_i}\right], & \text{se non differenziabile in } x_0
\end{cases}
\]

Nel contesto della regressione LASSO, questo si traduce nella seguente espressione:

\[
\text{sub} \nabla \mathbf{\Phi} = 
\begin{cases} 
\gamma \alpha + (u_j - x_j) & \text{se } u_j > 0 \\
-\gamma \alpha + (u_j - x_j) & \text{se } u_j < 0 \\
\left[-\gamma \alpha + (u_j - x_j), \gamma \alpha + (u_j - x_j)\right] & \text{se } u_j = 0
\end{cases}
\]
\[
\Rightarrow 
\begin{cases} 
u_j = x_j - \gamma \alpha & \text{se } x_j > \gamma \alpha \\
u_j = x_j + \gamma \alpha & \text{se } x_j < -\gamma \alpha \\
u_j = 0 & \text{se } -\gamma \alpha < x_j < \gamma \alpha
\end{cases}
\]

Questa operazione, nota come Soft-Thresholding, fornisce una soluzione analitica per l'aggiornamento dei coefficienti nel caso di penalità $L1$, dove il gradiente classico non è applicabile. \par
Il processo iterativo prosegue fino a quando la norma della differenza tra i coefficienti di due iterazioni successive diventa inferiore a una soglia di tolleranza prestabilita, indicando il raggiungimento della convergenza.

\subsection{ADMM}
L’Alternating Direction Method of Multipliers (ADMM) è un algoritmo di ottimizzazione primal-dual che risolve problemi complessi scomponendoli in una serie di sottoproblemi più semplici e gestibili, che vengono risolti in modo iterativo attraverso l'aggiornamento alternato delle variabili primali e duali.\\

Per applicare l'ADMM al problema LASSO, è necessario riformularlo introducendo una variabile ausiliaria (o \textit{slack variable}) $\mathbf{z}$. Questa scomposizione permette di separare l'ottimizzazione della funzione di loss originale dalla penalità $L1$, facilitando la risoluzione attraverso sottoproblemi indipendenti. \newpage

La formulazione risultante è la seguente:
\[
\min_{\mathbf{x}, \mathbf{z}} \frac{1}{2} \|A\mathbf{x} - \mathbf{y}\|_2^2 + \alpha \|\mathbf{z}\|_1 \quad 
\]
\[
\text{s.t.} \quad \mathbf{x} - \mathbf{z} = 0
\]
Il framework ADMM scompone il problema di ottimizzazione in tre distinti sottoproblemi, le cui soluzioni vengono calcolate in modo alternato e iterativo fino al soddisfacimento di un criterio di convergenza. \par
Di seguito viene presentata la formulazione standard in forma scalata dell'algoritmo:
\[
\mathbf{x}^{(k+1)} = \underset{\mathbf{x}}{\operatorname{argmin}} \left\{\frac{1}{2} \|A\mathbf{x} - \mathbf{y}\|_2^2 + \frac{\rho}{2} \|\mathbf{x} - \mathbf{z}^{(k)} + \mathbf{u}^{(k)}\|_2^2\right\}
\]
\[
\mathbf{z}^{(k+1)} = \underset{\mathbf{z}}{\operatorname{argmin}} \left\{\alpha \|\mathbf{z}\|_1 + \frac{\rho}{2} \|\mathbf{x}^{(k+1)} - \mathbf{z} + \mathbf{u}^{(k)}\|_2^2\right\}
\]
\[
\mathbf{u}^{(k+1)} = \mathbf{u}^{(0)} + \sum_{i=1}^{k+1} \|\mathbf{x}^{(i)} - \mathbf{z}^{(i)}\|_2^2
\]
Il primo step ammette una soluzione in forma chiusa, ottenuta ponendo a zero la derivata della funzione obiettivo:
\[
\nabla_\mathbf{x} \left(\frac{1}{2}\|A\mathbf{x} - \mathbf{y}\|_2^2 + \frac{\rho}{2}\|\mathbf{x} - \mathbf{z}^{(k)} + \mathbf{u}^{(k)}\|_2^2\right) = 0
\]
\[
A^T(A\mathbf{x} - \mathbf{y}) + \rho(\mathbf{x} - \mathbf{z}^{(k)} + \mathbf{u}^{(k)}) = 0
\]
\[
(A^TA + \rho I)\mathbf{x} = A^T\mathbf{y} + \rho(\mathbf{z}^{(k)} - \mathbf{u}^{(k)})
\]
\[
\rightarrow \mathbf{x}^{(k+1)} = (A^TA + \rho I)^{-1}(A^T\mathbf{y} + \rho(\mathbf{z}^{(k)} - \mathbf{u}^{(k)}))
\]
Per il secondo sottoproblema, la funzione obiettivo non è differenziabile a causa della norma $L1$. La sua soluzione, tuttavia, può essere ottenuta in forma chiusa imponendo le condizioni di sub-gradiente, le quali, per la norma $L1$, si traducono nell'applicazione dell'operatore di soft-thresholding.\\

In conclusione, la formulazione del problema LASSO mediante ADMM in forma scalata è la seguente:
\[
\mathbf{x}^{(k+1)} = (A^TA + \rho I)^{-1}(A^T\mathbf{y} + \rho(\mathbf{z}^{(k)} - \mathbf{u}^{(k)}))
\]
\[
\mathbf{z}^{(k+1)} = S_{\frac{\alpha}{\rho}}(\mathbf{x}^{(k+1)} + \mathbf{u}^{(k)})
\]
\[
\mathbf{u}^{(k+1)} = \mathbf{u}^{(k)} + \mathbf{x}^{(k+1)} - \mathbf{z}^{(k+1)}
\]

I parametri $\rho$ e $\alpha$ rivestono un ruolo critico nell'algoritmo, poiché la loro scelta influenza fondamentalmente il bilanciamento tra velocità di convergenza e robustezza della soluzione.\\

Nonostante questa sensibilità ai parametri, l'utilizzo dell'ADMM per il problema LASSO offre notevoli vantaggi. \par
Il suo vantaggio principale deriva dalla scomposizione del problema originale in sottoproblemi più semplici e gestibili, spesso risolvibili in forma chiusa. Questa architettura garantisce all'algoritmo una notevole stabilità e consente di raggiungere la soluzione ottimale in un numero di iterazioni generalmente contenuto. \par
Un ulteriore vantaggio decisivo è la sua capacità di essere parallelizzato e implementato in modo distribuito, che lo rende particolarmente adatto per scalare su problemi di grandi dimensioni.

\begin{comment}
Il criterio di convergenza si basa sul calcolo dei residui primali e duali. \par Considerando $n = \#$\textit{features} abbiamo che:
\[
r = \|\mathbf{x} - \mathbf{z}\| \quad \text{residuo primale}
\]
\[
s = \|-\rho(\mathbf{z}^{(k+1)} - \mathbf{z}^{(k)})\| \quad \text{residuo duale}
\]
\[
\epsilon_r = \sqrt{n}\epsilon + \epsilon_{\text{rel}}\max(\|\mathbf{x}\|, \|\mathbf{z}\|) \quad \text{tolleranza primale}
\]
\[
\epsilon_s = \sqrt{n}\epsilon + \epsilon_{\text{rel}}\|\rho u\| \quad \text{tolleranza duale}
\]
\[
\text{if } (r < \epsilon_r) \& (s < \epsilon_s) \rightarrow \text{break} \quad \text{criterio di convergenza}
\]
\end{comment}


\subsection{ADMM Distribuito}
La versione distribuita dell'ADMM estende il framework classico per gestire problemi su larga scala distribuendo il carico computazionale tra diversi nodi (o agenti). L'ottimizzazione del consenso viene implementata partizionando il dataset tra gli agenti, ciascuno dei quali mantiene una copia locale delle variabili di ottimizzazione. In questo modo, il problema globale viene scomposto rispetto ai dati e risolto in maniera collaborativa attraverso lo scambio di informazioni tra i nodi.\par
Questo approccio è possibile quando la funzione obiettivo presenta una struttura additiva rispetto ai dati, il che permette di scomporre il problema di ottimizzazione in sottoproblemi indipendenti che possono essere risolti in parallelo.\\


La formulazione standard del problema di ottimizzazione del consenso, in cui $N$ agenti devono minimizzare una funzione obiettivo globale scomponibile, è la seguente:
\[
\min_{(x_1, \dots, x_N), \mathbf{z}} \sum_{i=1}^N f_i(\mathbf{x}) + g(\mathbf{z}) \quad
\]
\[
\text{s.t.} \quad \mathbf{x}_i - \mathbf{z} = \mathbf{0}, \quad i = 1, \dots, N
\]

Il problema Lasso è intrinsicamente distribuibile grazie alla natura separabile della sua funzione obiettivo. Infatti, sia le osservazioni $y$ che la matrice $A$ possono essere partizionati in $N$ blocchi, rendendo possibile la scomposizione del problema originale in sottoproblemi indipendenti.

\[
\mathbf{y} = \begin{bmatrix}\mathbf{y}_1 \\ \vdots \\ \mathbf{y}_N\end{bmatrix}, \quad A\mathbf{x} = \begin{bmatrix}A_1 \\ \vdots \\ A_N\end{bmatrix}\mathbf{x} = \begin{bmatrix}A_1\mathbf{x} \\ \vdots \\ A_N\mathbf{x}\end{bmatrix}
\]

Andando a riformulare il problema nella forma di un'ottimizzazione al consenso, si ottiene la seguente rappresentazione:

\begin{comment}
In relazione a ADMM, per la formulazione distribuita si va ad eseguire uno split del dataset in base al numero di agenti a disposizione, in modo tale che ognuno possa calcolare la propria variabile di ottimizzazione $\mathbf{x}$ rispetto alla propria porzione di dati.
\end{comment}

\[
\min_{(x_1, \dots, x_N), \mathbf{z}} \sum_{i=1}^N \|\mathbf{y}_i - A_i\mathbf{x}_i\|_2^2 + \lambda \|\mathbf{z}\|_1 \quad 
\]
\[
\text{s.t.} \quad \mathbf{x}_i - \mathbf{z} = \mathbf{0}, \quad i = 1, \dots, N
\]

La soluzione viene ottenuta applicando la versione scalata dell'ADMM, la cui struttura è la seguente:
\[
\mathbf{x}_i^{(k+1)} = \underset{\mathbf{x}_i}{\operatorname{argmin}}\left\{\|\mathbf{y}_i - A_i\mathbf{x}_i\|_2^2 + \frac{\rho}{2}\|\mathbf{x}_i - \mathbf{z}^{(k)} + \mathbf{u}_i^{(k)}\|_2^2\right\}
\]
\[
\mathbf{z}^{(k+1)} = \underset{\mathbf{z}}{\operatorname{argmin}}\left\{g(\mathbf{z}) + \frac{\rho}{2}\sum_{i=1}^N \|\mathbf{x}_i^{(k+1)} - \mathbf{z} + \mathbf{u}_i^{(k)}\|_2^2\right\} \quad i=1,\ldots,N
\]
%\[= S_{\frac{\lambda}{N\rho}}(\hat{\mathbf{x}}^{(k+1)} + \hat{\mathbf{u}}^{(k)})\]
\[
\mathbf{u}_i^{(k+1)} = \mathbf{u}_i^{(k)} + (\mathbf{x}_i^{(k+1)} - \mathbf{z}^{(k+1)}) \quad i=1,\ldots,N
\]

Il primo step si compone di $N$ sottoproblemi indipendenti, ciascuno assegnabile a un agente distinto. Come nella formulazione centralizzata, questi ammettono una soluzione in forma chiusa e possono essere quindi calcolati in parallelo.\par
Il secondo step richiede la raccolta di tutte le variabili primali e duali locali per il calcolo di un valore di consenso globale. Questa operazione, non distribuibile, viene affidata a un fusion center.\par
Il terzo step può essere eseguito in modo completamente autonomo e parallelo da ciascun agente sulla base dei nuovi valori di $\mathbf{x}_i$ e $\mathbf{z}$.\\

Di conseguenza, le iterazioni dell'algoritmo per ogni agente $i$ e per il fusion center sono definite dalle seguenti equazioni:
\[
\mathbf{x}_i^{(k+1)} = (A_i^T A_i + \frac{\rho}{2} I)^{-1} (A_i^T \mathbf{y_i} + \frac{\rho}{2} (\mathbf{z}_i^{(k)} - \mathbf{u}_i^{(k)}))
\]
\[
\mathbf{z}^{(k+1)} = \text{Prox}_{\frac{\lambda}{N \rho} \| \cdot \|_1} \left\{\hat{\mathbf{x}}^{(k+1)} + \hat{\mathbf{u}}^{(k)} \right\} = S_{\frac{\lambda}{N \rho}} (\hat{\mathbf{x}}^{(k+1)} + \hat{\mathbf{u}}^{(k)})
\]
\[
\mathbf{u}_i^{(k+1)} = \mathbf{u}_i^{(k)}+\mathbf{x}_i^{(k+1)} - \mathbf{z}^{(k+1)} \quad i=1,\ldots,N
\]

dove $\hat{\mathbf{x}}^{(k+1)}$ e $\hat{\mathbf{u}}^{(k)}$ sono le medie delle variabili primarie e duali al passo $k+1$ e $k$ rispettivamente.\\

Questa tecnica è particolarmente adatta per l'elaborazione di dataset di grandi dimensioni e per scenari in cui i dati sono intrinsecamente distribuiti su più nodi computazionali.

\newpage

\section{Implementazione}
L’implementazione del confronto tra diversi algoritmi di regressione LASSO è stata realizzata interamente in MATLAB, sfruttando un approccio modulare e facilmente estendibile. Il progetto si compone di diversi script e classi che gestiscono la preparazione dei dati, la definizione e l'addestramento dei modelli, nonché la valutazione delle relative performance.

\subsection{Dataset}
Il dataset utilizzato in questo studio è il \href{https://www.kaggle.com/datasets/camnugent/california-housing-prices}{California Housing Dataset}, un insieme di dati pubblici ampiamente utilizzato nella letteratura di machine learning. Questo dataset contiene informazioni statistiche aggregate sul mercato immobiliare californiano, dove ogni osservazione rappresenta un blocco residenziale. Le feature disponibili includono sia variabili demografiche che proprietà fisiche delle abitazioni, in particolare:
\begin{itemize}
    \item \textbf{longitude}: Longitudine della posizione del blocco residenziale.
    \item \textbf{latitude}: Latitudine della posizione del blocco residenziale.
    \item \textbf{housing\_median\_age}: Età mediana delle abitazioni nel blocco residenziale.
    \item \textbf{total\_rooms}: Numero totale di stanze in tutte le abitazioni del blocco residenziale.
    \item \textbf{total\_bedrooms}: Numero totale di camere da letto in tutte le abitazioni del blocco residenziale.
    \item \textbf{population}: Popolazione residente nel blocco.
    \item \textbf{households}: Numero di nuclei familiari (households) nel blocco.
    \item \textbf{median\_income}: Reddito mediano delle famiglie nel blocco (in decine di migliaia di dollari).
    \item \textbf{median\_house\_value}: Valore mediano delle abitazioni nel blocco (in dollari USA).
    \item \textbf{ocean\_proximity}: Prossimità all’oceano, valore categorico (ad es. NEAR BAY, INLAND, $<1H$ OCEAN, ecc.).
\end{itemize}

\begin{comment}
Durante la fase di preprocessing, i valori mancanti nella variabile \textit{total\_bedrooms} sono stati sostituiti con la mediana. Tutte le feature numeriche sono state normalizzate nell’intervallo $[0, 1]$. La variabile categorica \textit{ocean\_proximity} è stata esclusa dall’analisi.
\end{comment}

\subsection{Preprocessing dei dati}
La fase iniziale prevede la pulizia e la normalizzazione del dataset California Housing. Lo script \texttt{preprocess\_california\_housing.m} si occupa di:
\begin{itemize}
    \item Caricare il dataset originale e gestire eventuali valori mancanti, sostituendo quelli presenti nella colonna \textit{total\_bedrooms} con la mediana.
    \item Escludere la colonna \textit{ocean\_proximity}, che rappresenta una variabile categorica, per focalizzare il modello su variabili numeriche.
    \item Selezionare le feature principali e la variabile target.
    \item Normalizzare le feature nell’intervallo $[0, 1]$ per garantire una migliore convergenza degli algoritmi di ottimizzazione.
    \item Salvare il dataset preprocessato in un nuovo file CSV, pronto per l’analisi.
\end{itemize}

\subsection{Classe LassoReg}
Gli algoritmi descritti in precedenza sono stati implementati all’interno della classe \texttt{LassoReg}, presente nel file \texttt{src/LassoReg.m}.\par

In fase di istanziazione, è possibile specificare diversi parametri, tra cui: l’algoritmo di ottimizzazione da utilizzare per la fase di training, il valore dello step-size, la tolleranza per il criterio di convergenza, il numero massimo di iterazioni e il coefficiente di penalizzazione $L1$.\par

La classe mette inoltre a disposizione metodi per la predizione sui nuovi dati, il calcolo della funzione di loss, e la visualizzazione della convergenza degli algoritmi di ottimizzazione.

\subsection{Script principale}
Il file \texttt{main.m} coordina le operazioni principali per l’analisi e il confronto degli algoritmi LASSO:
\begin{itemize}
    \item Carica il dataset preprocessato.
    \item Suddivide i dati in training e test set.
    \item Istanzia e addestra i modelli LASSO secondo i tre approcci.
    \item Valuta le performance tramite la metrica $R^2$ e visualizza graficamente sia le predizioni sia l’andamento della funzione obiettivo e dei residui.
    \item Consente di confrontare in modo diretto efficienza, rapidità di convergenza e accuratezza dei diversi algoritmi.
\end{itemize}

\subsection{Modularità ed estensibilità}
La struttura del codice è stata pensata per favorire la modularità: è possibile aggiungere nuovi algoritmi di ottimizzazione o modificare quelli esistenti con interventi minimi. L'uso di classi e funzioni dedicate permette di mantenere il codice chiaro e riutilizzabile.

\newpage


\section{Risultati e Confronto tra Algoritmi}

\subsection{Configurazione parametri e Tabella comparativa}
I parametri configurati per il confronto tra i tre algoritmi sono i seguenti:
\begin{itemize}
    \item Iterazioni massime = 50000;
    \item Step-size = 0.01;
    \item Penalità $L1$ = 1;
    \item Tolleranza = 1e-4;
    \item Agenti = 8 (ADMM distribuito).
\end{itemize}

La Tabella \ref{tab:confronto-prestazioni} riassume il confronto delle prestazioni ottenute.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
 & R2 & time (s) & iterazioni \\
\midrule
ISTA & 0.5588 & 12.876 & 50000 \\
ADMM & 0.5839 & 0.0010 & 4 \\
ADMM-Dist & 0.5726 & 0.0376 & 170 \\
\bottomrule
\end{tabular}
\caption{Comparazione algoritmi}
\label{tab:confronto-prestazioni}
\end{table}

È importante sottolineare che i valori riportati sono stati ottenuti effettuando una suddivisione casuale del dataset in training set e test set. Pertanto, esecuzioni successive potrebbero produrre risultati leggermente differenti.

\newpage
\input{capitoloGrafici}


\subsection{Discussione dei Risultati}
\subsubsection*{Precisione ($\mathbf{R^2}$)}
\begin{itemize}
    \item \textbf{ADMM} ottiene il valore di $R^2$ più alto ($0.5839$), indicando una migliore capacità predittiva rispetto agli altri algoritmi.
    \item \textbf{ADMM Distribuito} raggiunge un $R^2$ leggermente inferiore ($0.5726$), ma comunque superiore rispetto a ISTA.
    \item \textbf{ISTA} si attesta su un valore di $R^2$ di $0.5588$, mostrando una performance comunque valida ma inferiore agli altri metodi.
\end{itemize}

\subsubsection*{Tempo di esecuzione}
\begin{itemize}
    \item \textbf{ADMM} si distingue nettamente per la rapidità: solo $0.001$ secondi per raggiungere la convergenza, grazie all’efficienza della soluzione analitica dei sottoproblemi e alla rapida decrescita dei residui.
    \item \textbf{ADMM Distribuito} impiega più tempo ($0.0376 s$) rispetto all'ADMM classico, principalmente a causa della suddivisione del problema tra più agenti e della necessità di sincronizzazione, ma resta comunque molto più veloce di ISTA.
    \item \textbf{ISTA} è significativamente più lento ($12.876 s$), dovendo iterare il processo gradualmente a causa della natura dell’ottimizzazione tramite gradient descent e del trattamento della regolarizzazione $L1$.
\end{itemize}

\subsubsection*{Numero di iterazioni}
\begin{itemize}
    \item \textbf{ADMM} converge in pochissime iterazioni ($4$), dimostrando efficienza nella risoluzione del problema Lasso.
    \item \textbf{ADMM Distribuito} richiede più iterazioni ($170$), ma rimane estremamente efficiente rispetto a ISTA.
    \item \textbf{ISTA} ha raggiunto il numero massimo di iterazioni impostato ($50000$) senza riuscire a convergere, evidenziando una convergenza molto più lenta rispetto agli altri algoritmi.
\end{itemize}

\subsection{Conclusioni}
L’analisi dei risultati mostra chiaramente che l’algoritmo ADMM è nettamente superiore sia in termini di accuratezza che di efficienza computazionale, risultando ideale quando si dispone di risorse centralizzate e la dimensione del problema lo consente. \par
Il Distributed ADMM, pur essendo leggermente meno accurato e più lento del classico ADMM, rappresenta una valida soluzione in contesti distribuiti o quando i dati sono suddivisi tra più nodi. \par
L’algoritmo ISTA, pur essendo semplice da implementare, soffre di una convergenza lenta e di una minore accuratezza, risultando meno competitivo per problemi di dimensioni medio-grandi.\\

In sintesi, la scelta dell'algoritmo più adatto dipende dal contesto applicativo e dai vincoli computazionali: ADMM è preferibile per prestazioni, Distributed ADMM per scenari distribuiti, mentre ISTA può essere considerato per semplicità su dataset piccoli o come baseline.


\end{document}

\hyphenpenalty=0

\cleardoublepage\phantomsection % to fix wrong hyperref to \part{Epilogue}

\end{document}